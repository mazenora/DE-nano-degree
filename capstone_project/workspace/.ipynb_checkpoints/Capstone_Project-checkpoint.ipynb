{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Mapping Immigration Traffic with Airlines\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In The Project we will try to map Immigration TRaffic and Airlines data sets into a star schema for better analysis.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Import Needed Libraries\n",
    "import configparser\n",
    "import os\n",
    "from datetime import datetime, date, time, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, TimestampType, DateType, IntegerType\n",
    "from pyspark.sql.functions import udf, col, year, month, dayofmonth, weekofyear, date_format, row_number\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "We want to implement an ETL process to create an optimized data model to be effiecient enough to analyze the immigration traffic wiht Airlines using a star schema model.\n",
    "We will use Spark library to cleansing and transformaing the data sets and put the final results on S3 Bucket as parquet files.  \n",
    "> you need spark-sas7bdat-2.1.0-s_2.11.jar , parso-2.0.10.jar and Java 1.8\n",
    "\n",
    "#### Describe and Gather Data \n",
    "3 Datasets used in the Project:\n",
    "1. I94 Immigration Data: This data comes from the US National Tourism and Trade Office. and it includes international visitor arrival statistics on select countries, type of visa, mode of transportation, age groups, states visited (first intended address only). source website (https://travel.trade.gov/research/reports/i94/historical/2016.html)  \n",
    "2. Airlines Data: This dataset came from kaggel. You can read more about it here (https://www.kaggle.com/open-flights/airline-database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read AWS config \n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['aws']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['aws']['AWS_SECRET_ACCESS_KEY']\n",
    "\n",
    "# Read in the data here\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"immigration Temprature Analysis\") \\\n",
    "    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.0\" )\\\n",
    "    .config(\"spark.driver.memory\", \"5g\") \\\n",
    "    .config(\"spark.executor.memory\", \"5g\") \\\n",
    "    .config(\"spark.files.fetchTimeout\",\"3m\") \\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"3m\") \\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "\n",
    "# Read Airports Data\n",
    "dfAirlines = spark.read.format('csv').option('header', 'True').load(\"airlines.csv\")\n",
    "\n",
    "# To make it easy we read one file only\n",
    "dfImmigration = spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_aug16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----+----+----+----------+-------------+------+\n",
      "|Airline_ID|                Name|Alias|IATA|ICAO|  Callsign|      Country|Active|\n",
      "+----------+--------------------+-----+----+----+----------+-------------+------+\n",
      "|      5172|Open Skies Consul...|   \\N|  1L| OSY|OPEN SKIES|United States|     N|\n",
      "|      5002|           Tiara Air|   \\N|  3P| TNM|     TIARA|        Aruba|     Y|\n",
      "|     11963|         Starline.kz| null|  DZ|  \\N|     ALUNK|   Kazakhstan|     Y|\n",
      "+----------+--------------------+-----+----+----+----------+-------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleaning Airlines Data\n",
    "\n",
    "# Drop null values of IATA field\n",
    "dfAirlines = dfAirlines.dropna(how='any', subset=['IATA'])\n",
    "\n",
    "#Drop Duplicated IATA codes\n",
    "dfAirlines = dfAirlines.dropDuplicates(['IATA'])\n",
    "\n",
    "# Remove space from column name, cast type to Integer\n",
    "dfAirlines = dfAirlines.withColumnRenamed(\"Airline ID\",\"Airline_ID\")\n",
    "dfAirlines = dfAirlines.withColumn(\"Airline_ID\", dfAirlines.Airline_ID.cast(IntegerType()))\n",
    "\n",
    "#Create List of all IATA field codes, to be used as Filter with Immigration Dataset\n",
    "airline_codes = list(dfAirlines.select('IATA').toPandas()['IATA'])\n",
    "\n",
    "#show dataset\n",
    "dfAirlines.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+----------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|   arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+----------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "|   22| 2016|     8| 323.0| 323.0|    NYC|2016-08-01|    1.0|     FL|   null|  23.0|    3.0|  1.0|20160801|     RID| null|      U|   null|   null|   null| 1993.0|     D/S|     M|  null|     EK| 6.451049563E10|  201|      F1|\n",
      "|   55| 2016|     8| 209.0| 209.0|    AGA|2016-08-01|    1.0|     CA|   null|  41.0|    2.0|  1.0|20160801|    null| null|      A|   null|   null|   null| 1975.0|09142016|     M|  3955|     JL|5.7571868933E10|00941|     GMT|\n",
      "|   56| 2016|     8| 209.0| 209.0|    AGA|2016-08-01|    1.0|     GU|   null|  24.0|    2.0|  1.0|20160801|    null| null|      A|   null|   null|   null| 1992.0|09152016|     F|  3661|     UA|5.7571894533E10|00874|     GMT|\n",
      "+-----+-----+------+------+------+-------+----------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cleaning Immigration data\n",
    "\n",
    "# droping missing values in port, address and airline fields\n",
    "dfImmigration = dfImmigration.dropna(how='any', subset=['i94port', 'i94addr','airline'])\n",
    "\n",
    "# udf function to map invalid column values to other\n",
    "@udf(StringType())\n",
    "def validate_state(airline):  \n",
    "    if airline in airline_codes:\n",
    "        return airline\n",
    "    return 'other'\n",
    "\n",
    "#convert sas date integer to readable date \n",
    "@udf(DateType())\n",
    "def parse_date(arrdate):\n",
    "    if arrdate:\n",
    "        return (datetime(1960, 1, 1).date() + timedelta(int(arrdate)))\n",
    "    return None\n",
    "\n",
    "# extracting valid airline \n",
    "dfImmigration = dfImmigration.withColumn(\"airline\", validate_state(dfImmigration.airline))\n",
    "# extract arrival_date in standard format\n",
    "dfImmigration = dfImmigration.withColumn(\"arrdate\", parse_date(dfImmigration.arrdate))\n",
    "\n",
    "#cast columns to integer\n",
    "dfImmigration = dfImmigration.withColumn(\"cicid\", dfImmigration.cicid.cast(IntegerType()))\n",
    "dfImmigration = dfImmigration.withColumn(\"i94yr\", dfImmigration.i94yr.cast(IntegerType()))\n",
    "dfImmigration = dfImmigration.withColumn(\"i94mon\", dfImmigration.i94mon.cast(IntegerType()))\n",
    "\n",
    "#Show Dataset\n",
    "dfImmigration.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-----+---+----+-------+\n",
      "|arrive_date|year|month|day|week|weekday|\n",
      "+-----------+----+-----+---+----+-------+\n",
      "| 2016-08-15|2016|    8| 15|  33|    Mon|\n",
      "| 2016-08-31|2016|    8| 31|  35|    Wed|\n",
      "| 2016-08-23|2016|    8| 23|  34|    Tue|\n",
      "+-----------+----+-----+---+----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Date Dataframe\n",
    "dfDate = dfImmigration.select(\"arrdate\").dropDuplicates() \\\n",
    "        .withColumn(\"year\", year(col(\"arrdate\"))).withColumn(\"month\", month(col(\"arrdate\"))) \\\n",
    "        .withColumn(\"day\", dayofmonth(col(\"arrdate\"))).withColumn(\"week\", weekofyear(col(\"arrdate\"))) \\\n",
    "        .withColumn(\"weekday\", date_format(col(\"arrdate\"),'E')) \n",
    "dfDate = dfDate.withColumnRenamed(\"arrdate\",\"arrive_date\")\n",
    "\n",
    "#show Data\n",
    "dfDate.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "We will convert the Dataset to a Star schema as data model, so users can write simple queries by joing fact and dimension tables to analyze the data.\n",
    "\n",
    "Data model Diagram:\n",
    "<img src=\"ERD.png\">\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "There will be four Tables as following:\n",
    "- Fact Table (ImmigrationAirline): created by joining ImmigrationDim and AirlineDim Table --> saved as parquet file on S3 (partitioned by: Arrive_date and IATA_Code)\n",
    "- ImmigrationDim Dimension table: created from cleansed Immigration dataframe --> saved to a parquet file on s3, partitioned by (\"arrdate\",\"airline\")  \n",
    "- AirlineDim Dimension table:created from cleansed dataframe Airlines --> saved to a parquet file on s3.\n",
    "- DateDim Dimension table: created from Immigration(arrdate) dataframe column --> saved to a parquet file on s3.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "S3_output_path=\"s3a://udacity-ende-capstone/data-model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-----------+--------------+\n",
      "|IATA| cicid|arrive_date|ImmAirline_key|\n",
      "+----+------+-----------+--------------+\n",
      "|  2B| 13685| 2016-08-01|             1|\n",
      "|  2D|303516| 2016-08-02|             2|\n",
      "|  2D|404694| 2016-08-02|             3|\n",
      "+----+------+-----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create view & fact table\n",
    "dfImmigration.createOrReplaceTempView(\"immigrationDim\")\n",
    "dfAirlines.createOrReplaceTempView(\"airlineDim\")\n",
    "\n",
    "ImmigrationAirline_fact = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "           a.IATA,\n",
    "           i.cicid,\n",
    "           i.arrdate as arrive_date\n",
    "    FROM immigrationDim i\n",
    "    JOIN airlineDim a\n",
    "    ON i.airline == a.IATA\n",
    "\"\"\").dropDuplicates()\n",
    "\n",
    "# Add Serial Key\n",
    "window = Window.orderBy(col('IATA'))\n",
    "ImmigrationAirline_fact = ImmigrationAirline_fact.withColumn('ImmAirline_key', row_number().over(window))\n",
    "\n",
    "#Show Fact data\n",
    "ImmigrationAirline_fact.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write dimmension tables to s3\n",
    "dfImmigration.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(S3_output_path , 'immigrationDim.parquet')).\\\n",
    "    partitionBy(\"arrdate\", \"airline\")\n",
    "\n",
    "dfAirlines.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(S3_output_path , 'airlineDim.parquet'))\n",
    "    \n",
    "dfDate.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(S3_output_path , 'dateDim.parquet'))\n",
    "\n",
    "# write fact table to s3\n",
    "ImmigrationAirline_fact.\\\n",
    "    write.mode(\"overwrite\").\\\n",
    "    parquet(os.path.join(S3_output_path , 'ImmigrationAirline.parquet')).\\\n",
    "    partitionBy(\"arrive_date\", \"IATA\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "We can make sure that Fact table dataframe already has rows, and then query with dimension table to see if it is mapped correctly.  \n",
    "We can read the parquet files after writing them to make sure total rows are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+-----------+\n",
      "|IATA|dim_IATA|arrive_date|\n",
      "+----+--------+-----------+\n",
      "|  LH|      LH| 2016-08-01|\n",
      "|  AA|      AA| 2016-08-01|\n",
      "|  7C|      7C| 2016-08-01|\n",
      "|  7C|      7C| 2016-08-01|\n",
      "|  7C|      7C| 2016-08-01|\n",
      "+----+--------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make sure Dataframes has rows.\n",
    "fact_total = ImmigrationAirline_fact.count()\n",
    "if fact_total == 0:\n",
    "    print(\"Quality Check::: Your Fact Table has no Rows, Please Check!!!...\")\n",
    "\n",
    "# Check Fact data is mapped Correctly\n",
    "ImmigrationAirline_fact.createOrReplaceTempView(\"fact\")\n",
    "fact_airline = spark.sql(\"\"\"\n",
    "    select f.IATA,a.IATA as dim_IATA, f.arrive_date\n",
    "    from fact f\n",
    "    JOIN airlineDim a\n",
    "    ON f.IATA == a.IATA\n",
    "\"\"\")\n",
    "fact_airline.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# verify the data in parquet files (one for example)\n",
    "immigrationDim_parquet = spark.read.parquet(S3_output_path + \"immigrationDim.parquet\")\n",
    "print (\"Number of records in state table: \", immigrationDim_parquet.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 5: Complete Project Write Up\n",
    "\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.  \n",
    "We used Apache Spark to read, clean, transform, and create parquet files. Spark's schema-on-read is a powerful tool that let you do all the transformation without using any additional database. Using spark, We could process the raw data as if I am working on a traditional dtabase\n",
    "\n",
    "* Propose how often the data should be updated and why.  \n",
    "Since the Immigration data generated every month, then the ETL pipeline can be updated monthly.\n",
    "\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:  \n",
    "   * The data was increased by 100x.\n",
    "Probably we will run the job on Spark Cluster Solution like AWS EMR, and scale up the nodes if needed.  \n",
    "   * The data populates a dashboard that must be updated on a daily basis by 7am every day.  \n",
    "The ETL process (pipeline) may scheduled to run using orchestration tool like Apache Airflow on daily basis at 7:00 am.\n",
    "Other related resources Airlines can be scheduled on Yearly basis.\n",
    "   * The database needed to be accessed by 100+ people.  \n",
    "We can store data in cluster mode database like Cassandra or Redshift and scle up the nodes when needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
